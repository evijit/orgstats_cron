name: Daily HF Models Data Update (Modular)

on:
  schedule:
    - cron: '0 2 * * *'  # Run at 2 AM UTC daily
  workflow_dispatch:

env:
  ACTIONS_RUNNER_DEBUG: true
  ACTIONS_STEP_DEBUG: true

jobs:
  update-data:
    runs-on: ubuntu-latest
    timeout-minutes: 90  # Increased timeout for better reliability
    strategy:
      fail-fast: false

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Log system resources and environment
      run: |
        echo "=== System Information ==="
        echo "Timestamp: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
        echo "CPU cores: $(nproc)"
        echo "Memory total: $(free -h | grep 'Mem:' | awk '{print $2}')"
        echo "Memory available: $(free -h | grep 'Mem:' | awk '{print $7}')"
        echo "Disk space: $(df -h / | tail -1 | awk '{print $4}')"
        echo "Swap: $(free -h | grep 'Swap:' | awk '{print $2}')"
        echo "Python version: $(python --version)"
        echo "Working directory: $(pwd)"
        echo "Files in directory: $(ls -la)"
        echo ""
        echo "=== Python Environment ==="
        pip list | grep -E "(pandas|psutil|duckdb|pyarrow|tqdm)" || echo "Some packages not found"
        echo ""
        echo "=== Module Verification ==="
        python - <<EOF
        import sys
        modules_to_test = ['config', 'utils', 'data_fetcher', 'tag_processor', 'data_processor']
        all_passed = True
        for module in modules_to_test:
            try:
                __import__(module)
                print(f"‚úÖ {module}.py imported successfully")
            except ImportError as e:
                print(f"‚ùå {module}.py import failed: {e}")
                all_passed = False
        if not all_passed:
            sys.exit(1)
        EOF
        echo "=========================="

    - name: Test individual modules
      run: |
        echo "=== Testing Individual Modules ==="

        echo "üß™ Testing data_fetcher module..."
        # Use timeout and check the exit code of the python script
        if ! timeout 300s python - <<EOF; then
        from data_fetcher import fetch_raw_data, validate_raw_data
        from utils import log_progress
        import sys
        try:
            log_progress('Testing data fetcher...')
            df_raw, timestamp = fetch_raw_data()
            validate_raw_data(df_raw)
            log_progress(f'‚úÖ Data fetcher test passed - {len(df_raw):,} rows')
        except Exception as e:
            log_progress(f'‚ùå Data fetcher test failed: {e}')
            sys.exit(1) # Exit with error code to be caught by the shell
        EOF
            echo "‚ùå Data fetcher test failed"
        fi

        echo ""
        echo "üß™ Testing tag_processor module..."
        python tag_processor.py || echo "‚ùå Tag processor test failed"

        echo ""
        echo "üß™ Testing data_processor module..."
        python data_processor.py || echo "‚ùå Data processor test failed"

        echo "=== Module Testing Complete ==="

    - name: Run main processing pipeline
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        echo "=== Starting Main Data Processing Pipeline ==="
        echo "Start time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

        # Function to run the main pipeline with timeout and monitoring
        run_main_pipeline() {
          local attempt=$1
          echo ""
          echo "üîÑ Pipeline Execution (Attempt $attempt)"
          echo "======================================"

          # Run with timeout and capture output
          timeout 3600s python main.py 2>&1 | while IFS= read -r line; do
            echo "$(date -u '+%H:%M:%S'): $line"
          done

          return ${PIPESTATUS[0]}  # Return the exit code of python, not timeout
        }

        # Execute pipeline with retry logic
        max_attempts=2
        attempt=1

        while [ $attempt -le $max_attempts ]; do
          echo "üöÄ Starting pipeline attempt $attempt of $max_attempts"

          if run_main_pipeline $attempt; then
            echo ""
            echo "‚úÖ Pipeline completed successfully on attempt $attempt!"
            echo "Completion time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
            break
          else
            exit_code=$?
            echo ""
            echo "‚ùå Pipeline failed on attempt $attempt (exit code: $exit_code)"

            if [ $attempt -eq $max_attempts ]; then
              echo "üí• All pipeline attempts exhausted"
              exit $exit_code
            else
              echo "‚è≥ Preparing for retry..."
              echo "System resources before retry:"
              free -h
              df -h

              # Cleanup any temporary files
              rm -f models_processed.parquet.tmp *.tmp 2>/dev/null || true

              echo "Waiting 60 seconds before retry..."
              sleep 60
              attempt=$((attempt + 1))
            fi
          fi
        done

    - name: Detailed output verification
      id: detailed_verification
      run: |
        echo "=== Detailed Output Verification ==="
        echo "Verification time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

        if [ -f "models_processed.parquet" ]; then
          file_size=$(stat -c%s "models_processed.parquet")
          file_size_mb=$((file_size / 1024 / 1024))

          echo "‚úÖ Output file exists"
          echo "üìÅ File: models_processed.parquet"
          echo "üìä Size: ${file_size} bytes (${file_size_mb} MB)"

          if [ $file_size -gt 2097152 ]; then  # 2MB minimum
            echo "‚úÖ File size acceptable"

            # Comprehensive validation using a heredoc
            validation_result=$(python - <<EOF
        import pandas as pd
        import sys
        from datetime import datetime

        def log_with_timestamp(msg):
            print(f"[{datetime.utcnow().strftime('%H:%M:%S')}] {msg}")

        try:
            log_with_timestamp('üîç Loading and validating parquet file...')
            df = pd.read_parquet('models_processed.parquet')

            log_with_timestamp('‚úÖ File loaded successfully')
            log_with_timestamp(f'üìä Dataset shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns')
            log_with_timestamp(f'üíæ Memory usage: {df.memory_usage(deep=True).sum() / (1024*1024):.1f} MB')

            # Column verification
            expected_critical_cols = ['id', 'downloads', 'likes', 'params', 'organization', 'has_robot', 'size_category']
            missing_critical = [col for col in expected_critical_cols if col not in df.columns]

            if missing_critical:
                log_with_timestamp(f'‚ùå Missing critical columns: {missing_critical}')
                sys.exit(1)
            else:
                log_with_timestamp('‚úÖ All critical columns present')

            # Data quality checks
            log_with_timestamp('üìà Data Quality Summary:')

            unique_ids = df['id'].nunique()
            total_ids = len(df['id'].dropna())
            log_with_timestamp(f'   üÜî Model IDs: {total_ids:,} total, {unique_ids:,} unique')

            if 'has_robot' in df.columns:
                robot_count = df['has_robot'].sum()
                robot_pct = (robot_count / len(df)) * 100
                log_with_timestamp(f'   ü§ñ Robotics models: {robot_count:,} ({robot_pct:.2f}%)')
                if robot_count > 0:
                    robot_models = df[df['has_robot']]['id'].head(3).tolist()
                    log_with_timestamp(f'   ü§ñ Example robotics models: {robot_models}')

            if 'organization' in df.columns:
                org_count = df['organization'].nunique()
                top_org = df['organization'].value_counts().index[0]
                top_org_count = df['organization'].value_counts().iloc[0]
                log_with_timestamp(f'   üè¢ Organizations: {org_count:,} unique')
                log_with_timestamp(f'   üè¢ Top organization: {top_org} ({top_org_count:,} models)')

            if 'size_category' in df.columns:
                size_dist = df['size_category'].value_counts()
                log_with_timestamp('   üìè Size distribution:')
                for size_cat, count in size_dist.head(3).items():
                    pct = (count / len(df)) * 100
                    log_with_timestamp(f'      {size_cat}: {count:,} ({pct:.1f}%)')

            if 'data_download_timestamp' in df.columns:
                latest_timestamp = df['data_download_timestamp'].max()
                log_with_timestamp(f'   ‚è∞ Data timestamp: {latest_timestamp}')

            if len(df) < 100000:
                log_with_timestamp(f'‚ö†Ô∏è  Dataset smaller than expected: {len(df):,} rows')
            else:
                log_with_timestamp('‚úÖ Dataset size validation passed')

            log_with_timestamp('üéâ Comprehensive validation completed successfully!')

        except Exception as e:
            log_with_timestamp(f'‚ùå Validation failed: {str(e)}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        EOF
            )

            validation_exit_code=$?
            echo "${validation_result}"

            if [ $validation_exit_code -eq 0 ]; then
              echo "has_changes=true" >> $GITHUB_OUTPUT
              echo "file_size_mb=${file_size_mb}" >> $GITHUB_OUTPUT
            else
              echo "‚ùå Data validation failed"
              echo "has_changes=false" >> $GITHUB_OUTPUT
              exit 1
            fi
          else
            echo "‚ùå File size too small (${file_size_mb} MB)"
            echo "has_changes=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        else
          echo "‚ùå Output file missing"
          echo "has_changes=false" >> $GITHUB_OUTPUT
          exit 1
        fi

    - name: Upload to Hugging Face Space
      if: steps.detailed_verification.outputs.has_changes == 'true'
      run: |
        echo "=== Uploading to Hugging Face Space ==="
        echo "Upload start: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

        upload_with_retry() {
          local attempt=$1
          local max_attempts=$2

          echo "üì§ Upload attempt $attempt of $max_attempts"

          python - <<EOF
        from huggingface_hub import HfApi
        import os
        from datetime import datetime
        import sys
        import time

        def log_upload(msg):
            print(f"[{datetime.utcnow().strftime('%H:%M:%S')}] {msg}")

        try:
            api = HfApi()
            timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')
            file_path = 'models_processed.parquet'
            file_size_mb = os.path.getsize(file_path) / (1024*1024)

            log_upload(f'üì§ Starting upload of {file_size_mb:.1f}MB file')
            log_upload(f'üéØ Target: evijit/OrgStats space')
            log_upload(f'‚è∞ Timestamp: {timestamp}')

            start_time = time.time()

            result = api.upload_file(
                path_or_fileobj=file_path,
                path_in_repo='models_processed.parquet',
                repo_id='evijit/OrgStats',
                repo_type='space',
                token=os.environ['HF_TOKEN'],
                commit_message=f'Automated data update - {timestamp}'
            )

            upload_time = time.time() - start_time
            log_upload(f'‚úÖ Upload completed successfully in {upload_time:.1f}s')
            log_upload(f'üîó Result: {result}')

        except Exception as e:
            log_upload(f'‚ùå Upload failed: {str(e)}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        EOF
          return $?
        }

        # Retry logic for upload
        max_upload_attempts=3
        upload_attempt=1

        while [ $upload_attempt -le $max_upload_attempts ]; do
          if upload_with_retry $upload_attempt $max_upload_attempts; then
            echo "‚úÖ Upload successful on attempt $upload_attempt"
            echo "Upload completion: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
            break
          else
            exit_code=$?
            echo "‚ùå Upload failed on attempt $upload_attempt"

            if [ $upload_attempt -eq $max_upload_attempts ]; then
              echo "üí• All upload attempts failed"
              exit $exit_code
            else
              echo "‚è≥ Waiting 45 seconds before retry..."
              sleep 45
              upload_attempt=$((upload_attempt + 1))
            fi
          fi
        done

    - name: Create workflow summary
      if: always()
      run: |
        echo "=== Workflow Execution Summary ==="
        echo "Summary generated: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

        # Check the job conclusion status
        if [[ "${{ job.status }}" == "success" ]]; then
          file_size=$(stat -c%s "models_processed.parquet")
          file_size_mb=$((file_size / 1024 / 1024))

          echo "### üéâ SUCCESS: Data Processing Completed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| üìä Output File Size | ${file_size_mb} MB |" >> $GITHUB_STEP_SUMMARY
          echo "| ‚è±Ô∏è Total Execution Time | $SECONDS seconds |" >> $GITHUB_STEP_SUMMARY
          echo "| üïí Completion Time | $(date -u '+%Y-%m-%d %H:%M:%S UTC') |" >> $GITHUB_STEP_SUMMARY

          # Add data quality info from the parquet file
          python - <<EOF >> $GITHUB_STEP_SUMMARY
        import pandas as pd
        try:
            df = pd.read_parquet('models_processed.parquet')
            print(f"| ü§ñ Total Models | {len(df):,} |")
            if 'has_robot' in df.columns:
                robot_count = df['has_robot'].sum()
                print(f"| ü¶æ Robotics Models | {robot_count:,} |")
            if 'organization' in df.columns:
                org_count = df['organization'].nunique()
                print(f"| üè¢ Organizations | {org_count:,} |")
        except:
            pass # Suppress errors if file is missing/corrupt
        EOF

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "‚úÖ **Data successfully uploaded to Hugging Face Space: evijit/OrgStats**" >> $GITHUB_STEP_SUMMARY
        else
          echo "### ‚ùå FAILURE: Data Processing Failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The data processing pipeline did not complete successfully." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| ‚è±Ô∏è Failed After | $SECONDS seconds |" >> $GITHUB_STEP_SUMMARY
          echo "| üïí Failure Time | $(date -u '+%Y-%m-%d %H:%M:%S UTC') |" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "*Automated workflow run [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})*" >> $GITHUB_STEP_SUMMARY

    - name: Upload build artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: pipeline-outputs-${{ github.run_number }}
        path: |
          models_processed.parquet
          *.log
        retention-days: 7
        if-no-files-found: warn

    - name: System cleanup and final status
      if: always()
      run: |
        echo "=== Final System Status ==="
        echo "Cleanup time: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

        # Show final resource usage
        echo "üìä Final Resource Usage:"
        echo "   Memory: $(free -h | grep 'Mem:' | awk '{print $3 "/" $2}')"
        echo "   Disk: $(df -h / | tail -1 | awk '{print $3 "/" $2}')"

        # List output files
        echo ""
        echo "üìÅ Generated Files:"
        ls -lah *.parquet 2>/dev/null || echo "   No parquet files found"
        ls -lah *.log 2>/dev/null || echo "   No log files found"

        # Cleanup temporary files
        echo ""
        echo "üßπ Cleaning up temporary files..."
        rm -f *.tmp 2>/dev/null || true
        rm -f data_meta.txt 2>/dev/null || true

        # Final status based on job status
        echo ""
        if [[ "${{ job.status }}" == "success" ]]; then
          file_size_mb=$(( $(stat -c%s "models_processed.parquet") / 1024 / 1024 ))
          echo "üéâ WORKFLOW COMPLETED SUCCESSFULLY!"
          echo "   ‚úÖ Output file: models_processed.parquet (${file_size_mb} MB)"
          echo "   ‚úÖ Total time: $SECONDS seconds"
          echo "   ‚úÖ Status: Data uploaded to HuggingFace Space"
        else
          echo "‚ùå WORKFLOW FAILED!"
          echo "   ‚ùå Check job logs for error details"
          echo "   ‚ùå Total time: $SECONDS seconds"
        fi

        echo ""
        echo "=== End of Workflow ==="