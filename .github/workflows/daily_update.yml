name: Daily HF Data Update (Models & Datasets)

on:
  schedule:
    - cron: '0 2 * * *'  # Run at 2 AM UTC daily
  workflow_dispatch:

jobs:
  update-models-data:
    name: "Update Daily Models Data"
    runs-on: ubuntu-latest
    timeout-minutes: 90
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Test Models pipeline on a small data subset
      env:
        TEST_DATA_LIMIT: 5000
      run: python test_pipeline.py

    - name: Run Models processing pipeline (full dataset)
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: timeout 3600s python -u main.py

    - name: Verify Models output file
      id: verify_models
      run: |
        if [ ! -f "models_processed.parquet" ]; then exit 1; fi
        file_size_mb=$(($(stat -c%s "models_processed.parquet") / 1024 / 1024))
        if [ $file_size_mb -lt 2 ]; then exit 1; fi
        echo "file_path=models_processed.parquet" >> $GITHUB_OUTPUT
        echo "has_changes=true" >> $GITHUB_OUTPUT

    - name: Upload Models data to Hugging Face
      if: steps.verify_models.outputs.has_changes == 'true'
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        pip install huggingface_hub
        huggingface-cli upload evijit/modelverse_daily_data \
          ${{ steps.verify_models.outputs.file_path }} \
          ${{ steps.verify_models.outputs.file_path }} \
          --repo-type dataset --commit-message "Automated models data update" --token $HF_TOKEN

  update-datasets-data:
    name: "Update Daily Datasets Data"
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Test Datasets pipeline on a small data subset
      env:
        TEST_DATA_LIMIT: 5000
      run: python test_pipeline_datasets.py

    - name: Run Datasets processing pipeline (full dataset)
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: timeout 2400s python -u main_datasets.py

    - name: Verify Datasets output file
      id: verify_datasets
      run: |
        if [ ! -f "datasets_processed.parquet" ]; then exit 1; fi
        file_size_mb=$(($(stat -c%s "datasets_processed.parquet") / 1024 / 1024))
        if [ $file_size_mb -lt 1 ]; then exit 1; fi
        echo "file_path=datasets_processed.parquet" >> $GITHUB_OUTPUT
        echo "has_changes=true" >> $GITHUB_OUTPUT

    - name: Upload Datasets data to Hugging Face
      if: steps.verify_datasets.outputs.has_changes == 'true'
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        pip install huggingface_hub
        huggingface-cli upload evijit/dataverse_daily_data \
          ${{ steps.verify_datasets.outputs.file_path }} \
          ${{ steps.verify_datasets.outputs.file_path }} \
          --repo-type dataset --commit-message "Automated datasets data update" --token $HF_TOKEN