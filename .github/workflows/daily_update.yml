name: Daily HF Data Update (Models, Datasets & Papers)

on:
  schedule:
    - cron: '0 2 * * *'  # Run at 2 AM UTC daily
  workflow_dispatch:

jobs:
  update-models-data:
    name: "Update Daily Models Data"
    runs-on: ubuntu-latest
    timeout-minutes: 90
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Test Models pipeline on a small data subset
      env:
        TEST_DATA_LIMIT: 5000
      run: python test_pipeline.py

    - name: Run Models processing pipeline (full dataset)
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: timeout 3600s python -u main.py

    - name: Verify Models output file
      id: verify_models
      run: |
        if [ ! -f "models_processed.parquet" ]; then exit 1; fi
        file_size_mb=$(($(stat -c%s "models_processed.parquet") / 1024 / 1024))
        if [ $file_size_mb -lt 2 ]; then exit 1; fi
        echo "file_path=models_processed.parquet" >> $GITHUB_OUTPUT
        echo "has_changes=true" >> $GITHUB_OUTPUT

    - name: Upload Models data to Hugging Face
      if: steps.verify_models.outputs.has_changes == 'true'
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        pip install huggingface_hub
        hf upload evijit/modelverse_daily_data \
          ${{ steps.verify_models.outputs.file_path }} \
          ${{ steps.verify_models.outputs.file_path }} \
          --repo-type dataset --commit-message "Automated models data update"

  update-datasets-data:
    name: "Update Daily Datasets Data"
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Test Datasets pipeline on a small data subset
      env:
        TEST_DATA_LIMIT: 5000
      run: python test_pipeline_datasets.py

    - name: Run Datasets processing pipeline (full dataset)
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: timeout 2400s python -u main_datasets.py

    - name: Verify Datasets output file
      id: verify_datasets
      run: |
        if [ ! -f "datasets_processed.parquet" ]; then exit 1; fi
        file_size_mb=$(($(stat -c%s "datasets_processed.parquet") / 1024 / 1024))
        if [ $file_size_mb -lt 1 ]; then exit 1; fi
        echo "file_path=datasets_processed.parquet" >> $GITHUB_OUTPUT
        echo "has_changes=true" >> $GITHUB_OUTPUT

    - name: Upload Datasets data to Hugging Face
      if: steps.verify_datasets.outputs.has_changes == 'true'
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        pip install huggingface_hub
        hf upload evijit/dataverse_daily_data \
          ${{ steps.verify_datasets.outputs.file_path }} \
          ${{ steps.verify_datasets.outputs.file_path }} \
          --repo-type dataset --commit-message "Automated datasets data update"

  papers-setup:
    name: "Setup Papers Processing"
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.split.outputs.matrix }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Fetch paper count and calculate job split
      id: split
      run: |
        # Fetch data to get count
        PAPER_COUNT=$(python -c "from data_fetcher_papers import fetch_raw_data; df = fetch_raw_data(); print(len(df))")
        echo "Total papers: $PAPER_COUNT"
        
        # Calculate job split and show info
        python split_citation_jobs.py $PAPER_COUNT
        
        # Generate matrix for parallel jobs
        MATRIX=$(python generate_matrix.py $PAPER_COUNT)
        echo "Matrix: $MATRIX"
        echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  fetch-citations:
    name: "Fetch Citations (Job ${{ matrix.job_id }})"
    needs: papers-setup
    runs-on: ubuntu-latest
    timeout-minutes: 330
    strategy:
      matrix:
        include: ${{ fromJson(needs.papers-setup.outputs.matrix) }}
      fail-fast: false
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Fetch citations for batch
      run: |
        python fetch_citations_batch.py ${{ matrix.start_idx }} ${{ matrix.end_idx }}

    - name: Upload batch artifact
      uses: actions/upload-artifact@v4
      with:
        name: citations-batch-${{ matrix.job_id }}
        path: citations_batch_*.parquet
        retention-days: 1

  update-papers-data:
    name: "Process Papers with Taxonomy & Merge Citations"
    needs: [papers-setup, fetch-citations]
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Download spaCy model
      run: |
        python -m spacy download en_core_web_lg

    - name: Download all citation batch artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: citations-batch-*
        merge-multiple: true

    - name: Merge citation batches
      run: |
        python merge_citation_batches.py

    - name: Test Papers pipeline on a small data subset
      env:
        TEST_DATA_LIMIT: 50
        ENABLE_CITATION_FETCHING: false
      run: python test_pipeline_papers.py

    - name: Run Papers processing pipeline (taxonomy only, citations from merged file)
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        ENABLE_CITATION_FETCHING: false
      run: |
        # Process papers with taxonomy (skip citation fetching, we have them merged)
        timeout 3600s python -u main_papers.py
        
        # Merge the citation data into the final output
        python merge_citations_into_papers.py

    - name: Verify Papers output files
      id: verify_papers
      run: |
        if [ ! -f "papers_with_semantic_taxonomy.parquet" ]; then exit 1; fi
        file_size_mb=$(($(stat -c%s "papers_with_semantic_taxonomy.parquet") / 1024 / 1024))
        if [ $file_size_mb -lt 1 ]; then exit 1; fi
        echo "parquet_file=papers_with_semantic_taxonomy.parquet" >> $GITHUB_OUTPUT
        echo "csv_file=papers_with_semantic_taxonomy.csv" >> $GITHUB_OUTPUT
        echo "has_changes=true" >> $GITHUB_OUTPUT

    - name: Upload Papers data to Hugging Face
      if: steps.verify_papers.outputs.has_changes == 'true'
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        pip install huggingface_hub
        # Upload parquet and metadata files (CSV removed - parquet is more efficient)
        hf upload evijit/paperverse_daily_data \
          papers_with_semantic_taxonomy.parquet \
          papers_with_semantic_taxonomy.parquet \
          --repo-type dataset --commit-message "Automated papers data update with taxonomy and citations"
        
        hf upload evijit/paperverse_daily_data \
          taxonomy_report.txt \
          taxonomy_report.txt \
          --repo-type dataset --commit-message "Automated papers data update with taxonomy and citations"
        
        hf upload evijit/paperverse_daily_data \
          taxonomy_distribution.json \
          taxonomy_distribution.json \
          --repo-type dataset --commit-message "Automated papers data update with taxonomy and citations"