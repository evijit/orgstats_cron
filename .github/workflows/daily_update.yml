name: Daily HF Models Data Update

on:
  schedule:
    # Runs at 2:00 AM UTC every day (adjust time as needed)
    - cron: '0 2 * * *'
  workflow_dispatch: # Allows manual triggering

env:
  ACTIONS_RUNNER_DEBUG: true
  ACTIONS_STEP_DEBUG: true

jobs:
  update-data:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Increased timeout
    strategy:
      fail-fast: false
      max-attempts: 3  # Retry the entire job up to 3 times
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Log system resources and limits
      run: |
        echo "=== System Information ==="
        echo "CPU cores: $(nproc)"
        echo "Memory total: $(free -h | grep 'Mem:' | awk '{print $2}')"
        echo "Memory available: $(free -h | grep 'Mem:' | awk '{print $7}')"
        echo "Disk space: $(df -h / | tail -1 | awk '{print $4}')"
        echo "Swap: $(free -h | grep 'Swap:' | awk '{print $2}')"
        echo "Ulimits:"
        ulimit -a
        echo "Python memory limit test:"
        python -c "
import psutil
import os
print(f'Available memory: {psutil.virtual_memory().available / (1024**3):.1f} GB')
print(f'Process limit: {psutil.Process().memory_info().rss / (1024**2):.1f} MB')
"
        echo "=========================="

    - name: Download and validate data
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        echo "=== Step 1: Data Download ==="
        python -c "
import duckdb
import pandas as pd
import time
import os

start_time = time.time()
HF_PARQUET_URL = 'https://huggingface.co/datasets/cfahlgren1/hub-stats/resolve/main/models.parquet'

print('üîÑ Downloading data from Hugging Face...')
try:
    query = f\"SELECT COUNT(*) as row_count FROM read_parquet('{HF_PARQUET_URL}')\"
    row_count = duckdb.sql(query).df().iloc[0]['row_count']
    print(f'‚úÖ Data validation successful: {row_count:,} rows available')
    
    # Download a sample first to estimate memory needs
    if row_count > 100000:
        print('üìä Large dataset detected, downloading sample for memory estimation...')
        sample_query = f\"SELECT * FROM read_parquet('{HF_PARQUET_URL}') LIMIT 10000\"
        sample_df = duckdb.sql(sample_query).df()
        
        # Estimate memory usage
        sample_memory = sample_df.memory_usage(deep=True).sum()
        estimated_full_memory = (sample_memory * row_count) / 10000
        print(f'üíæ Estimated memory needed: {estimated_full_memory / (1024**3):.2f} GB')
        
        if estimated_full_memory > 2 * (1024**3):  # > 2GB
            print('‚ö†Ô∏è  Large memory requirement detected - will use streaming approach')
        
        del sample_df
    
    print(f'‚è±Ô∏è  Data validation completed in {time.time() - start_time:.2f}s')
    
    # Save metadata for next step
    with open('data_meta.txt', 'w') as f:
        f.write(f'{row_count}\\n{estimated_full_memory if \"estimated_full_memory\" in locals() else 0}')
        
except Exception as e:
    print(f'‚ùå Data download validation failed: {e}')
    exit(1)
"

    - name: Process data with streaming approach
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      timeout-minutes: 40  # Set timeout for this step specifically
      run: |
        echo "=== Step 2: Data Processing ==="
        python -c "
import pandas as pd
import numpy as np
import duckdb
import time
import gc
import os
import psutil
from tqdm.auto import tqdm

# Read metadata
with open('data_meta.txt', 'r') as f:
    lines = f.read().strip().split('\\n')
    total_rows = int(lines[0])
    estimated_memory = float(lines[1]) if len(lines) > 1 else 0

print(f'üìã Processing {total_rows:,} rows (estimated memory: {estimated_memory/(1024**3):.2f} GB)')

HF_PARQUET_URL = 'https://huggingface.co/datasets/cfahlgren1/hub-stats/resolve/main/models.parquet'

# Use very small chunks for memory efficiency
chunk_size = 25000 if total_rows > 100000 else total_rows
print(f'üîÑ Using chunk size: {chunk_size:,} rows')

def get_memory():
    return psutil.Process().memory_info().rss / (1024**2)  # MB

def process_small_chunk(chunk_df):
    '''Minimal processing for memory efficiency'''
    try:
        # Only keep essential columns initially
        essential_cols = ['id', 'downloads', 'downloadsAllTime', 'likes', 'pipeline_tag', 'tags']
        if 'params' in chunk_df.columns:
            essential_cols.append('params')
        elif 'safetensors' in chunk_df.columns:
            essential_cols.append('safetensors')
        
        # Filter to essential columns only
        available_cols = [col for col in essential_cols if col in chunk_df.columns]
        chunk_small = chunk_df[available_cols].copy()
        
        # Basic data type optimization
        if 'downloads' in chunk_small.columns:
            chunk_small['downloads'] = pd.to_numeric(chunk_small['downloads'], errors='coerce').fillna(0.0)
        if 'downloadsAllTime' in chunk_small.columns:
            chunk_small['downloadsAllTime'] = pd.to_numeric(chunk_small['downloadsAllTime'], errors='coerce').fillna(0.0)
        if 'likes' in chunk_small.columns:
            chunk_small['likes'] = pd.to_numeric(chunk_small['likes'], errors='coerce').fillna(0.0)
        
        return chunk_small
        
    except Exception as e:
        print(f'‚ùå Chunk processing error: {e}')
        return None

# Process in streaming fashion
processed_chunks = []
memory_start = get_memory()
print(f'üöÄ Starting processing (initial memory: {memory_start:.1f} MB)')

try:
    for i in tqdm(range(0, total_rows, chunk_size), desc='Processing chunks'):
        chunk_start_time = time.time()
        
        # Download just this chunk
        offset = i
        limit = min(chunk_size, total_rows - i)
        
        query = f\"SELECT * FROM read_parquet('{HF_PARQUET_URL}') LIMIT {limit} OFFSET {offset}\"
        chunk_raw = duckdb.sql(query).df()
        
        # Process chunk
        chunk_processed = process_small_chunk(chunk_raw)
        if chunk_processed is None:
            raise Exception(f'Failed to process chunk {i//chunk_size + 1}')
        
        processed_chunks.append(chunk_processed)
        
        # Cleanup
        del chunk_raw, chunk_processed
        gc.collect()
        
        # Memory check
        current_memory = get_memory()
        chunk_time = time.time() - chunk_start_time
        
        print(f'‚úÖ Chunk {i//chunk_size + 1}/{(total_rows-1)//chunk_size + 1} done in {chunk_time:.1f}s (memory: {current_memory:.1f} MB)')
        
        # Safety check - if memory is growing too much, exit early
        if current_memory > memory_start + 1000:  # 1GB increase
            print(f'‚ö†Ô∏è  Memory usage increased significantly, stopping processing')
            break
    
    # Combine results
    print('üîÑ Combining processed chunks...')
    if processed_chunks:
        df_final = pd.concat(processed_chunks, ignore_index=True)
        del processed_chunks
        gc.collect()
        
        print(f'‚úÖ Combined data shape: {df_final.shape}')
        print(f'üìä Memory after combining: {get_memory():.1f} MB')
        
        # Add timestamp
        df_final['data_download_timestamp'] = pd.Timestamp.now(tz='UTC')
        
        # Save with compression
        print('üíæ Saving processed data...')
        df_final.to_parquet('models_processed.parquet', index=False, compression='snappy')
        
        file_size = os.path.getsize('models_processed.parquet') / (1024**2)
        print(f'‚úÖ Saved models_processed.parquet ({file_size:.1f} MB)')
        
        del df_final
        gc.collect()
    else:
        raise Exception('No chunks were processed successfully')
        
except Exception as e:
    print(f'‚ùå Processing failed: {e}')
    print(f'üìä Final memory usage: {get_memory():.1f} MB')
    exit(1)

print(f'üéâ Processing completed successfully!')
"

    - name: Verify output file
      id: check_changes
      run: |
        if [ -f "models_processed.parquet" ]; then
          file_size=$(stat -c%s "models_processed.parquet")
          echo "‚úÖ File exists: models_processed.parquet (${file_size} bytes)"
          
          # Check if file is reasonable size (> 500KB)
          if [ $file_size -gt 512000 ]; then
            echo "‚úÖ File size looks reasonable"
            echo "has_changes=true" >> $GITHUB_OUTPUT
            
            # Quick validation
            python -c "
import pandas as pd
try:
    df = pd.read_parquet('models_processed.parquet')
    print(f'‚úÖ File validation: {len(df):,} rows, {len(df.columns)} columns')
    if len(df) > 0:
        print('‚úÖ Data validation passed')
    else:
        print('‚ùå No data in file')
        exit(1)
except Exception as e:
    print(f'‚ùå File validation failed: {e}')
    exit(1)
"
          else
            echo "‚ùå File size too small, might be corrupted"
            echo "has_changes=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        else
          echo "‚ùå File does not exist - processing failed"
          echo "has_changes=false" >> $GITHUB_OUTPUT
          exit 1
        fi

    - name: Upload to Hugging Face Space with retry
      if: steps.check_changes.outputs.has_changes == 'true'
      run: |
        echo "=== Step 3: Upload to Hugging Face ==="
        
        for attempt in {1..3}; do
          echo "üîÑ Upload attempt $attempt/3"
          
          if python -c "
from huggingface_hub import HfApi
import os
from datetime import datetime

try:
    api = HfApi()
    timestamp = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')
    
    print(f'üì§ Uploading to evijit/OrgStats at {timestamp}')
    
    api.upload_file(
        path_or_fileobj='models_processed.parquet',
        path_in_repo='models_processed.parquet',
        repo_id='evijit/OrgStats',
        repo_type='space',
        token=os.environ['HF_TOKEN'],
        commit_message=f'Update models data - {timestamp}'
    )
    
    print('‚úÖ Upload successful!')
    
except Exception as e:
    print(f'‚ùå Upload failed: {e}')
    exit(1)
"; then
            echo "‚úÖ Upload completed successfully on attempt $attempt"
            break
          else
            if [ $attempt -eq 3 ]; then
              echo "‚ùå All upload attempts failed"
              exit 1
            else
              echo "‚è≥ Waiting 15 seconds before retry..."
              sleep 15
            fi
          fi
        done

    - name: Upload artifact as backup
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: models-processed-parquet-${{ github.run_number }}
        path: models_processed.parquet
        retention-days: 7

    - name: Final cleanup and status
      if: always()
      run: |
        echo "=== Final Status ==="
        if [ -f "models_processed.parquet" ]; then
          ls -lh models_processed.parquet
          echo "‚úÖ Parquet file exists"
        fi
        
        echo "=== Final System State ==="
        free -h
        df -h
        
        # Cleanup temporary files
        rm -f data_meta.txt
        
        echo "üéâ Workflow completed!"