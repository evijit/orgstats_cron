name: Papers Final Merge (After All Waves)

on:
  workflow_dispatch:

jobs:
  merge-all-citations:
    name: "Merge All Citation Waves"
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Download all citation artifacts from all waves
      uses: actions/download-artifact@v4
      with:
        pattern: citations-wave-*
        merge-multiple: true

    - name: Merge all citation batches
      run: |
        python merge_citation_batches.py

    - name: Upload merged citations artifact
      uses: actions/upload-artifact@v4
      with:
        name: citations-merged-all-waves
        path: papers_citations.parquet
        retention-days: 7

  update-papers-data:
    name: "Process Papers with Taxonomy & Merge Citations"
    needs: merge-all-citations
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Download spaCy model
      run: |
        python -m spacy download en_core_web_lg

    - name: Download merged citations
      uses: actions/download-artifact@v4
      with:
        name: citations-merged-all-waves

    - name: Test Papers pipeline on a small data subset
      env:
        TEST_DATA_LIMIT: 50
        ENABLE_CITATION_FETCHING: false
      run: python test_pipeline_papers.py

    - name: Run Papers processing pipeline (taxonomy only, citations from merged file)
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        ENABLE_CITATION_FETCHING: false
      run: |
        # Process papers with taxonomy (skip citation fetching, we have them merged)
        timeout 3600s python -u main_papers.py
        
        # Merge the citation data into the final output
        python merge_citations_into_papers.py

    - name: Verify Papers output files
      id: verify_papers
      run: |
        if [ ! -f "papers_with_semantic_taxonomy.parquet" ]; then exit 1; fi
        file_size_mb=$(($(stat -c%s "papers_with_semantic_taxonomy.parquet") / 1024 / 1024))
        if [ $file_size_mb -lt 1 ]; then exit 1; fi
        echo "parquet_file=papers_with_semantic_taxonomy.parquet" >> $GITHUB_OUTPUT
        echo "has_changes=true" >> $GITHUB_OUTPUT

    - name: Upload Papers data to Hugging Face
      if: steps.verify_papers.outputs.has_changes == 'true'
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        pip install huggingface_hub
        # Upload parquet and metadata files
        hf upload evijit/paperverse_daily_data \
          papers_with_semantic_taxonomy.parquet \
          papers_with_semantic_taxonomy.parquet \
          --repo-type dataset --commit-message "Automated papers data update with taxonomy and citations (all waves merged)"
        
        hf upload evijit/paperverse_daily_data \
          taxonomy_report.txt \
          taxonomy_report.txt \
          --repo-type dataset --commit-message "Automated papers data update with taxonomy and citations (all waves merged)"
        
        hf upload evijit/paperverse_daily_data \
          taxonomy_distribution.json \
          taxonomy_distribution.json \
          --repo-type dataset --commit-message "Automated papers data update with taxonomy and citations (all waves merged)"
